{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef704a5-964a-4578-a263-79424c250c49",
   "metadata": {},
   "source": [
    "#Q1\n",
    "Both the Probability Mass Function (PMF) and the Probability Density Function (PDF) are mathematical concepts used to describe the probabilities or likelihoods associated with different values of a random variable. However, they are used for different types of random variables: the PMF is used for discrete random variables, while the PDF is used for continuous random variables.\n",
    "\n",
    "Probability Mass Function (PMF):\n",
    "The PMF is a function that gives the probability of a discrete random variable taking on a specific value. It maps each possible value of the random variable to its associated probability.\n",
    "The PMF is used for discrete random variables and gives the probability of each possible outcome. It is defined as follows:\n",
    "PMF(x) = P(X = x)\n",
    "where X is the random variable and x is a possible outcome.\n",
    "The PMF returns the probability of X being equal to x.\n",
    "For example, suppose we have a fair six-sided die. The PMF for this die would be\n",
    "PMF(1) = 1/6\n",
    "PMF(2) = 1/6\n",
    "PMF(3) = 1/6\n",
    "PMF(4) = 1/6\n",
    "PMF(5) = 1/6\n",
    "PMF(6) = 1/6\n",
    "This means that the probability of rolling a 1 is 1/6, the probability of rolling a 2 is 1/6, and so on.\n",
    "The PMF is a discrete function because the possible outcomes (1, 2, 3, 4, 5, and 6) are all distinct and countable.\n",
    "\n",
    "The PDF, on the other hand, is used for continuous random variables and gives the density of probability at each possible value of the variable. It is defined as follows:\n",
    "PDF(x) = dF(x) / dx\n",
    "where F(x) is the cumulative distribution function (CDF) of the variable.\n",
    "The PDF returns the density of probability at a particular value of X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af2094-56be-4118-9783-a821aebea4db",
   "metadata": {},
   "source": [
    "#Q2\n",
    "The Cumulative Distribution Function (CDF) is a mathematical function used to describe the probability distribution of a random variable.\n",
    "The CDF is defined as the probability that the variable takes on a value less than or equal to a given value. In other words, it gives the cumulative probability of the random variable up to a certain point.\n",
    "\n",
    "Mathematically, \n",
    "F(x)=P(X<=x)\n",
    "where \n",
    "X is the random variable and \n",
    "x is a possible value.\n",
    "\n",
    "example-\n",
    "\n",
    "X={1,2,3,4,5,6}\n",
    "\n",
    " This is uniform distribution with p = 1/6 \n",
    "-calculating probability of dice showing number less than or equal to 3 then\n",
    "\n",
    "F(3)=P(1)+P(2)+P(3)=1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978eebc-580e-4bc7-98a0-b5802f11fa72",
   "metadata": {},
   "source": [
    "#Q3\n",
    "The normal distribution, also known as the Gaussian distribution or bell curve, is a versatile and widely used probability distribution in various fields due to its mathematical properties and its tendency to describe many natural phenomena. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "Height of Individuals: The heights of a large population of individuals tend to follow a normal distribution. This makes the normal distribution useful for predicting the likelihood of certain height ranges.\n",
    "\n",
    "Measurement Errors: In scientific experiments, measurement errors often follow a normal distribution. This makes it a convenient choice for modeling uncertainties in measurements.\n",
    "\n",
    "Test Scores: In standardized testing, the scores of a large group of test-takers often approximate a normal distribution. This allows educators to set grading curves and interpret scores easily.\n",
    "\n",
    "Financial Returns: Daily changes in stock prices or financial returns can often be modeled using a normal distribution, making it valuable in risk analysis and portfolio management.\n",
    "\n",
    "IQ Scores: Intelligence quotient (IQ) scores in a population tend to follow a normal distribution, with most people clustering around the average score.\n",
    "\n",
    "Natural Phenomena: Many natural phenomena, like the distribution of speeds of gas molecules in a container (according to the kinetic theory of gases), can be approximated using a normal distribution.\n",
    "\n",
    "The shape of the normal distribution is characterized by two parameters: the mean (μ) and the standard deviation (σ). These parameters have a significant impact on the distribution's appearance:\n",
    "\n",
    "Mean (μ): The mean is the center of the distribution, and it defines the peak of the bell curve. Shifting the mean to the left or right will move the entire distribution along the x-axis without changing the shape.\n",
    "\n",
    "Standard Deviation (σ): The standard deviation controls the spread or width of the distribution. A larger standard deviation results in a wider and flatter curve, while a smaller standard deviation leads to a narrower and taller curve.\n",
    "\n",
    "Together, the mean and standard deviation determine the location and shape of the distribution. A larger standard deviation implies more variability in the data, while a smaller standard deviation implies less variability. The normal distribution is symmetric around the mean, and its shape remains the same regardless of the mean and standard deviation values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be111aff-e524-4f70-b69d-c49bdc3e9cd6",
   "metadata": {},
   "source": [
    "#Q4\n",
    "The normal distribution, also known as the Gaussian distribution or bell curve, holds immense importance in various fields due to its numerous properties and its ability to model a wide range of real-world phenomena. Its significance stems from the fact that it appears naturally in many situations and serves as a fundamental building block for statistical analyses and hypothesis testing. Here are a few key reasons for the importance of the normal distribution:\n",
    "\n",
    "Central Limit Theorem: The central limit theorem states that the distribution of the sum (or average) of a large number of independent and identically distributed random variables will tend toward a normal distribution, regardless of the underlying distribution of the variables. This theorem underlies many statistical techniques, allowing analysts to make inferences about populations even when their distributions are not normal.\n",
    "\n",
    "Statistical Inference: Many statistical methods and hypothesis tests, such as t-tests and Z-tests, are based on the assumption that the data follows a normal distribution. This simplifies the analysis process and allows researchers to apply well-established techniques to their data.\n",
    "\n",
    "Parameter Estimation: Normal distribution plays a crucial role in parameter estimation, where methods like maximum likelihood estimation (MLE) and least squares estimation are often employed assuming a normal distribution of errors.\n",
    "\n",
    "Risk Analysis: In finance and risk management, the normal distribution is often used to model the distribution of asset returns. This allows for the calculation of measures like value at risk (VaR) and the construction of risk models.\n",
    "\n",
    "Quality Control: In manufacturing and quality control, normal distribution is used to analyze process variations and to set tolerance limits for products.\n",
    "\n",
    "Psychometrics: In psychological and educational testing, the normal distribution is utilized to set norms, define cutoff scores, and interpret test results.\n",
    "\n",
    "Social Sciences: Various social science phenomena, like human height, IQ scores, and standardized test scores, often exhibit normal distribution tendencies. This makes it easier to analyze and understand these characteristics.\n",
    "\n",
    "Natural Phenomena: Many natural phenomena, such as the distribution of speeds of gas particles in a container or the distribution of errors in measurements, follow the normal distribution due to the central limit theorem.\n",
    "\n",
    "Examples of real-life situations that can be modeled using the normal distribution include:\n",
    "\n",
    "Heights of a large population.\n",
    "IQ scores.\n",
    "Blood pressure readings in a healthy population.\n",
    "Exam scores in a well-designed test.\n",
    "Errors in measurements due to various factors.\n",
    "Daily stock market returns.\n",
    "The normal distribution's versatility and wide applicability make it a cornerstone of statistics and a powerful tool for understanding and analyzing data across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c55373-c402-4774-bde2-372b99484706",
   "metadata": {},
   "source": [
    "#Q5\n",
    "The Bernoulli distribution is a simple and fundamental discrete probability distribution that describes the outcomes of a binary or two-sided experiment, where there are only two possible outcomes: success (usually denoted as 'S') with probability p and failure (usually denoted as 'F') with probability 1−p. It is named after Jacob Bernoulli, a Swiss mathematician.\n",
    "\n",
    "Mathematically, the Bernoulli distribution is often represented as follows:\n",
    "P(X=S)=p\n",
    "P(X=F)=1-p\n",
    "\n",
    "Where:\n",
    "\n",
    "X is a random variable representing the outcome of the experiment.\n",
    "\n",
    "P(X=S) is the probability of success.\n",
    "\n",
    "P(X=F) is the probability of failure.\n",
    "\n",
    "p is the probability of success and 1−p is the probability of failure.\n",
    "\n",
    "Example of Bernoulli Distribution\n",
    "Consider the experiment of flipping a fair coin. If we define \"heads\" as success ('S') and \"tails\" as failure ('F'), then the outcome of each coin flip follows a Bernoulli distribution with p=0.5. The probability of getting heads is 0.5, and the probability of getting tails is also 0.5.\n",
    "\n",
    "Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "The Bernoulli distribution is a special case of the binomial distribution. Here's how they differ:\n",
    "\n",
    "Number of Trials:\n",
    "\n",
    "Bernoulli Distribution: Describes a single trial or experiment with two possible outcomes.\n",
    "Binomial Distribution: Describes the number of successes in a fixed number (n) of independent Bernoulli trials.\n",
    "Number of Possible Outcomes:\n",
    "\n",
    "Bernoulli Distribution: Only two possible outcomes (success and failure).\n",
    "Binomial Distribution: Counts the number of successes among n trials, so it can take on multiple values from 0 to n.\n",
    "Parameters:\n",
    "\n",
    "Bernoulli Distribution: Has a single parameter p (probability of success).\n",
    "Binomial Distribution: Has two parameters: n (number of trials) and p (probability of success).\n",
    "Probability Mass Function (PMF):\n",
    "\n",
    "Bernoulli Distribution: The PMF gives the probabilities of two possible outcomes.\n",
    "Binomial Distribution: The PMF gives the probabilities of getting k successes in n trials.\n",
    "Mean and Variance:\n",
    "\n",
    "In both cases, the mean (μ) and variance (σ^2 ) are related to n and p, but they have different formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a992cb3-cc85-414f-bd18-3a67ace9cd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of an observation being greater than 60 is: 0.15866\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "from scipy.stats import norm\n",
    "\n",
    "mean = 50\n",
    "std_dev = 10\n",
    "observed_value = 60\n",
    "\n",
    "z_score = (observed_value - mean) / std_dev\n",
    "\n",
    "probability = 1 - norm.cdf(z_score)\n",
    "\n",
    "print(\"The probability of an observation being greater than 60 is:\", round(probability,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25811055-0657-4098-bc4d-cf046d6283e1",
   "metadata": {},
   "source": [
    "#Q7\n",
    "Uniform distribution is a probability distribution where every possible outcome has an equal chance of occurring. This means that the values of a random variable are spread evenly over a given interval or range.\n",
    "A classic example of a uniform distribution is the roll of a fair die. The die has six faces, each labeled with a number from 1 to 6, and all the faces are equally likely to come up when the die is rolled. Therefore, the probability of each outcome is 1/6, and this distribution is called a discrete uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5496cf-f930-4d2d-80d2-961a7bf0c0fd",
   "metadata": {},
   "source": [
    "#Q8\n",
    "The z-score, also known as the standard score or the normalized score, is a statistical measurement that quantifies the number of standard deviations a data point is from the mean of a dataset. It's used to understand how far a particular data point is from the mean in terms of standard deviation units. The formula to calculate the z-score for a data point x in a dataset with mean μ and standard deviation σ is:\n",
    "z= x−μ/σ\n",
    "\n",
    "Where:\n",
    "z is the z-score of the data point.\n",
    "x is the value of the data point.\n",
    "μ is the mean of the dataset.\n",
    "σ is the standard deviation of the dataset.\n",
    "\n",
    "Importance of the z-score:\n",
    "The z-score is important for several reasons:\n",
    "\n",
    "Standardization: The z-score standardizes data, allowing you to compare and analyze data points on a common scale regardless of their original units or the scale of the distribution. This makes it easier to identify outliers and patterns in the data.\n",
    "\n",
    "Outlier Detection: Extreme z-scores (far from 0) often correspond to outliers, which are data points that are unusually distant from the rest of the data. They might indicate errors or interesting observations.\n",
    "\n",
    "Normalization: Z-scores normalize data, transforming it into a distribution with a mean of 0 and a standard deviation of 1. This is particularly useful for certain statistical techniques that assume a normal distribution or when comparing data from different sources.\n",
    "\n",
    "Probability and Percentiles: The z-score can be used to calculate the probability of observing a data point or a range of data points within a normal distribution. It's also used to find percentiles (e.g., the 95th percentile) in the standard normal distribution.\n",
    "\n",
    "Comparisons: Z-scores allow direct comparisons between data points from different datasets, as they are measured in terms of standard deviations.\n",
    "\n",
    "Hypothesis Testing: In hypothesis testing, the z-score is used to determine the significance of the difference between sample means, helping to make decisions about statistical hypotheses.\n",
    "\n",
    "Data Transformation: Z-scores are used in data preprocessing and feature scaling for machine learning algorithms to ensure that features are on similar scales, preventing certain features from having disproportionately large influences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315edd9-6677-4148-9da5-c964d7b8ddcb",
   "metadata": {},
   "source": [
    "#Q9\n",
    "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics that states that the sampling distribution of the sum (or average) of a large number of independent, identically distributed random variables will tend toward a normal distribution, regardless of the underlying distribution of the variables themselves. In simpler terms, when you take a large enough sample from any population, the distribution of the sample means will be approximately normal, regardless of the original distribution of the population.\n",
    "\n",
    "The Central Limit Theorem is particularly important and widely used due to its far-reaching implications and practical applications:\n",
    "\n",
    "Significance of the Central Limit Theorem:\n",
    "\n",
    "Approximation of Normality: The CLT provides a rationale for why the normal distribution is so commonly observed in real-world data, even when the underlying population distribution may not be normal. This approximation of normality is immensely helpful for statistical analysis and hypothesis testing.\n",
    "\n",
    "Inferential Statistics: The CLT allows us to make inferences about a population's mean based on a sample, even when the population distribution is unknown. This is the foundation of many hypothesis tests, confidence intervals, and parameter estimation techniques.\n",
    "\n",
    "Small Sample Sizes: Even when dealing with small sample sizes, the CLT indicates that if the sample size is sufficiently large, the distribution of sample means will be approximately normal. This allows us to apply statistical methods that assume normality.\n",
    "\n",
    "Variance Stabilization: The CLT can stabilize the variability of the sample mean. As the sample size increases, the standard deviation of the sample mean decreases, making the estimate of the population mean more precise.\n",
    "\n",
    "Aggregating Data: The CLT allows us to aggregate data from different sources or scenarios and still analyze it using normal-based statistical methods.\n",
    "\n",
    "Parameter Estimation: The CLT justifies the use of methods like the t-distribution for parameter estimation, where the population standard deviation is estimated from the sample.\n",
    "\n",
    "Quality Control: In quality control processes, the CLT allows for control charts to be used effectively, even when the underlying process distribution is not normal.\n",
    "\n",
    "Statistical Process Control: The CLT is crucial for statistical process control, where normality is often assumed to monitor and control processes effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5fa60-8abc-4c4b-bbe7-b2075eac9474",
   "metadata": {},
   "source": [
    "#Q10\n",
    "The key assumptions of the Central Limit Theorem are:\n",
    "\n",
    "Independence: The individual observations within each sample must be independent of each other. This means that the outcome of one observation should not influence the outcome of another observation.\n",
    "\n",
    "Identical Distribution: The random variables being sampled must be identically distributed. This means that the underlying population from which the samples are drawn should have the same probability distribution for each observation.\n",
    "\n",
    "Finite Variance: The random variables being sampled should have a finite variance (a measure of variability). If the variance is infinite, the CLT might not hold.\n",
    "\n",
    "Sample Size: The larger the sample size (n), the better the approximation to the normal distribution. While there's no strict rule for a \"large\" sample size, a common guideline is n≥30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e35c98-cab8-45ce-9a4b-d39c698bd964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
